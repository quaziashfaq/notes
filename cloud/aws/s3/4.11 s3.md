# S3 Exam Tips
These are the exam tips in ACG course.

## What's S3?
- S3 is object-based storage.
- It allows to upload and store files in the cloud.
- It's not suitable for operating systems or database storage.
- It's suitable for files that are up to 5 terabytes in size.
- It's unlimited storage.

## S3 Namespace
- Files are stored in buckets. 
- S3 is a universal namespace.
- When a bucket is created, the bucket name will always be in the URL.
- The naming convention is "<bucket-name>.s3.<region-name>.amazonaws.com
- And then / and then the key name.
- Here the key name is just the file name
- When a file is uploaded, browser will always get a HTTP 200 status code.


## S3 Objects
just remember that the key is the object name,
so it's the file name.
So Ralphie.jpg.
The value is the data itself,
which is made up of a sequence of bytes.
We then have our Version ID.
We looked at that when we were doing versioning,
and this allows you to have multiple versions
of the same object.
And then we always have metadata in S3,
and this is just data about your data.
So data about the data that you're storing.
So it could be the content type, for example,
if it's an image, or when it was last modified, etc.
So moving on to securing your buckets with S3.
Remember that S3 buckets are private by default.
When you create an S3 bucket, it's private,
and all the objects within it are private.
And you have to go in and allow public access
on both the bucket and its objects
in order to make the bucket public.
Remember that you have object ACLs,
and this is basically access control lists
that you apply on individual objects using object ACLs.
This is a way of giving permissions
to make individual objects public.
Or you could put an object ACL saying
you're not allowed to delete this object.
We then have bucket policies.
Bucket policies are bucket-wide policies,
so you can make entire buckets public using bucket policies.
And I've already covered this off, but like I said,
when you upload an object two S3 and it's successful
you'll always get a HTTP 200 status code.
Remember that you can host a static website with S3.
You do this using the bucket policies, it's best practice.
So basically you create a bucket policy
that makes every object within your bucket public.
And this is basically used just for static content,
it's not used for dynamic content.
So if you need a database connection using S3,
you don't want to run your websites in S3,
you would then use something like EC2.
So, it's static content only.
And remember that S3 automatically scales with demands.
You don't have to worry about
things like load balances, etc.
S3 is a great place to store your static websites with.
So moving on to versioning.
So we looked at how we can use versioning
with our S3 buckets, and all versions of an object
are going to be stored in an S3.
This includes all rights.
So we have our webpage where we had
version 1, version 2, version 3.
And even if you delete an object,
all that's doing is placing a delete marker
over that object version.
And then you can basically remove this
by deleting the delete marker.
So versioning can be a great backup tool.
It can't be disabled.
Once you turn versioning on, it can only be suspended.
You can't go in and turn it off.
And just remember
you can use versioning with lifecycle rules.
So you can integrate it with lifecycle rules,
so you can basically move your older versions
to different storage tiers,
and it supports multi-factor authentication as well.
Moving on to our different storage tiers,
this will come up an awful lot in your exam.
Just know the 6 different storage tiers
and know their different use cases.
So S3 Standard is suitable for most workloads.
So websites, content distribution,
mobile and gaming applications.
S3 Standard Infrequently Accessed is good for
long-term, infrequently accessed but critical data.
So this could be your backups,
your data store for your disaster recovery, etc.
S3 One Zone-Infrequent Access is great
for long-term, infrequently accessed but non-critical data
because it is only going to be in one availability zone.
And they might even give you scenario questions
where it says "Company policy dictates
"that it must be stored in at least
"2 or more availability zones."
Then straightaway you can rule out
S3 One Zone-Infrequently Access storage
as a storage class in that scenario.
We then have Glacier and Glacier Deep Archive.
Just remember the retrieval times.
So, essentially, if you need it before 12 hours,
then you want S3 Glacier.
If you're okay to wait an average of 12 hours or more,
then you want Deep Archive.
If you want to save the most money,
then of course you always want Deep Archive.
And then we have a S3 Intelligent-Tiering.
This basically just uses machine learning
to move your objects between the different tiers
to save you the most amount of money,
and it's used for unknown or unpredictable access patterns.
So, moving on to Lifecycle Management.
So 3 tips for Lifecycle Management.
This basically automates the moving
of your objects between the different storage tiers.
It can be used in conjunction with versioning,
and it can be applied to current versions
and previous versions of your object.
We then looked at S3 Object Lock to store objects
using a write once, read many model.
So as soon as you see the term WORM
and it's talking about S3,
I want you to think of S3 Object Lock.
And this can be on individual objects
or applied across the bucket as a whole.
And it always comes in 2 modes,
it comes in governance mode and compliance mode.
And just a gentle reminder of what each one of those is,
so with governance mode, users can't overwrite
or delete an object version after its lock settings
unless they have special permission.
So you can do it with certain number of users,
but if you need to ban all users
from being able to access or to be able to write
and delete those objects, then you want compliance mode.
This basically stops anyone from doing it
including the root accounts,
or the root user within your AWS account.
If you see a WORM model but it's not talking about S3,
it's talking about Glacier,
then you want S3 Glacier Vault Lock.
This allows you to easily deploy
and enforce compliance controls
for individual S3 Glacier Vaults with a Vault Lock Policy.
And you can specify controls such as a WORM model
in a Vault Lock Policy,
and lock the policy from future edits.
And once locked, the policy can no longer be changed.
Moving on to encryption with S3.
So we had 2 different types of ways we can encrypt data.
First of all is sending the data to S3.
So this is Encryption in Transit.
So we can encrypt our data using SSL or using HTTPS.
So whenever you visit S3 using your browser,
you're going to visit using HTTPS,
and that means all the objects that you send to S3
will be encrypted in transit.
We then have encryption at rest
using server-side encryption.
So that's all SSE stands for is server-side encryption.
And it comes in three different flavors.
We've got server-side encryption with S3,
and this is using the AES 256-bit encryption algorithm.
So this is where Amazon handles all
of the encryption on our behalf for us using the S3 service.
We can then also use a external service from S3,
which is Key Management Service,
to encrypt our data as well.
And then we've also got it where the customer
handles the keys themselves.
So this is SSE-C.
And then we've got client-side encryption.
And this is basically where you just
encrypt the files yourself before you upload them to S3.
And you can enforce encryption with a bucket policy.
So a bucket policy can deny all PUT requests
that don't include the x-amz
service-side-encryption parameter in the request header.
Moving on to optimizing performance with S3.
So we looked at prefixes.
So a prefix is simply the folder
and then subfolder within a S3 bucket.
So in this example we've got folder1
and then subfolder1, that is one prefix.
Remember that you can achieve a high number of requests.
So 3,500 PUT/COPY/POST/DELETES,
and then 5,500 GET and HEADER requests
per second per prefix.
And of course, the more prefixes you have,
the better performance that you get.
So if you have 2 prefixes, so 2 different prefixes,
you could then achieve an 11,000 request per second
for any GET request because it's spread over 2 prefixes.
If you did it across 4 prefixes,
you're going to be getting 22,000 requests per second.
So that's all prefixes are.
Just think of prefixes as folders and subfolders within S3.
The more of them that you have,
the better performance that you get.
Also remember that if you are using SSE,
server-side encryption, KMS to encrypt your objects in S3,
you must keep in mind that there are built-in limits.
The built-in limits are region specific,
but it's either going to be 5,500
10,000 or 30,000 requests per second.
Uploading and downloading is always going to count
towards your KMS quota.
And currently, you can't request an increase for KMS.
Remember, if we want to optimize our performance,
we can use multipart uploads to increase performance
when uploading to S3.
This should be used for any files over 100 megs,
but it must be used for any file over five gigs.
And similarly, if we are downloading data,
we can use S3 byte-range fetches to increase performance
to download our files from S3.
And this is basically just splitting the files
into smaller versions,
and then parallelizing the downloads.
And then finally, moving on to S3 Replication.
Just remember what S3 replication is,
this is where you can replicate objects
from 1 bucket to another.
It used to be that you would do this across region.
So if the exam questions haven't been updated,
they could call it Cross Region Replication,
but you can also do it to buckets in the same region
as well as different regions.
Just remember when you turn this on
that objects in an existing bucket
are not going to be replicated automatically.
And by default, delete markers are not replicated
automatically from 1 bucket to another,
but you can turn that on as an option.
So that is it for this section.
You've done really, really well.
We're going to move on to EC2,
which is one of the bread-and-butter services of AWS.
It's a huge topic in the exam,
and it's basically just where you provision virtual machines
in the Cloud.
So if you've got the time,
please join me in the next section.
Thank you.
